# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an MCP (Model Context Protocol) server written in Go that enables LLMs to get a second opinion on their responses by using another LLM as a judge. The server is distributed both as a Go binary and as an npm package (which wraps the binary).

## Build and Development Commands

### Building
```bash
# Build the binary
go build -o bin/llm-as-judge-mcp-server ./cmd/llm-as-judge-mcp-server

# Run locally
go run ./cmd/llm-as-judge-mcp-server stdio --provider=<provider> --model=<model>
```

### Testing the Server
```bash
# Test with different providers
go run ./cmd/llm-as-judge-mcp-server stdio --provider=gemini
go run ./cmd/llm-as-judge-mcp-server stdio --provider=anthropic --model=claude-sonnet-4
go run ./cmd/llm-as-judge-mcp-server stdio --provider=ollama --model=llama2
go run ./cmd/llm-as-judge-mcp-server stdio --provider=openai --prompt-path=/path/to/custom/prompt.md
```

### Release Process
```bash
# Create a release using goreleaser
goreleaser release --clean

# Test the release process locally
goreleaser release --snapshot --clean
```

## Architecture

### Core Components

1. **MCP Server Setup** (`cmd/llm-as-judge-mcp-server/main.go`)
   - Uses Cobra for CLI commands
   - Exposes a single `stdio` command that starts the MCP server
   - Required flag: `--provider` (anthropic, gemini, ollama, openai)
   - Optional flags: `--model`, `--prompt-path`

2. **Client Provider Pattern** (`internal/client/`)
   - `BaseClient[T]`: Generic base implementation using Go generics
   - Provider-specific clients: `AnthropicClient`, `GeminiClient`, `OllamaClient`, `OpenAIClient`
   - Each implements the `LLMClient` interface with a `Judge()` method
   - `GetClientProvider()`: Factory function that returns the appropriate client based on provider
   - `GetDefaultProviderModel()`: Maps providers to default models (see [README.md](README.md) for defaults)

3. **Handler** (`internal/handler/judge.go`)
   - `JudgeHandler`: Wraps the LLM client and handles MCP tool requests
   - `NewTool()`: Defines the MCP tool schema with required parameters (question, response) and optional (evaluation_focus)
   - `Handle()`: Processes incoming tool calls and returns results

4. **Prompts** (`internal/prompts/`)
   - Uses Go embed directive (`//go:embed`) to bundle default prompts into the binary
   - Default prompt is in `LLM_AS_JUDGE.md`
   - Users can override with custom prompts via `--prompt-path`

5. **Configuration** (`internal/config/config.go`)
   - Loads API keys from environment variables based on provider
   - Format: `{PROVIDER}_API_KEY` (e.g., `ANTHROPIC_API_KEY`, `GEMINI_API_KEY`)
   - Ollama doesn't require an API key

### Key Design Patterns

- **Factory Pattern**: `GetClientProvider()` creates the appropriate client based on provider string
- **Generic Base Class**: `BaseClient[T]` uses Go generics to avoid code duplication across providers
- **Embedded Resources**: Prompts are embedded at compile time using `//go:embed`
- **Interface-based Design**: All clients implement `LLMClient` interface for easy swapping

### Provider Implementation Details

When adding a new provider:
1. Create a new file in `internal/client/` (e.g., `newprovider.go`)
2. Implement the `LLMClient` interface with a `Judge()` method
3. Embed `BaseClient[T]` where T is the provider's client type
4. Add case to `GetClientProvider()` switch statement
5. Add default model to `GetDefaultProviderModel()`
6. Update `internal/config/config.go` if API key handling differs from standard pattern

### Distribution

The project has dual distribution:
- **Go binary**: Built with goreleaser for multiple platforms (Linux, macOS, Windows, multiple architectures)
- **npm package**: The `package.json` points to the binary in `bin/` directory, allowing installation via `npx @jgsheppa/llm-as-judge-mcp-server`

### MCP Tool Schema

The server exposes a single tool named `judge_response` with:
- **question** (required): The question posed to an LLM
- **response** (required): The response generated by an LLM
- **evaluation_focus** (optional): Specific areas to emphasize in the evaluation
